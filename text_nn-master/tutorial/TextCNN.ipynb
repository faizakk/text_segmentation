{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with CNN  (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction\n",
    "\n",
    "Text classification is one of the most common Natural Language Processing tasks. It consists in encoding a text into a tensor that can be then used by a machine learning method to predict a label, which may represent any category.\n",
    "\n",
    "Text classification has been used for a wide range of purposes, such as text categorization, spam detection, information extraction, sentiment analysis, and so on.\n",
    "\n",
    "In this short tutorial, we will develop a simple Convolutional Neural Network (CNN) for text classification in PyTorch. All code will be commented to increase readability.\n",
    "\n",
    "We will train and test our model on the 20 Newsgroup Dataset following the steps below:\n",
    " \n",
    "1. Load the word embeddings (e.g. Word2Vec or Glove)\n",
    "2. Load the dataset (i.e. 20 Newsgroup Dataset)\n",
    "3. Define the hyperparameters (e.g. arguments)\n",
    "4. Define the model (i.e. CNN)\n",
    "5. Train and Validate the model (Epochs, Metrics, etc.)\n",
    "6. Test the model (Metrics)\n",
    "\n",
    "The reader can expand and re-adapt our system to work on different datasets and for different purposes.\n",
    "\n",
    "\n",
    "### 1.1 References and Acknowledgements\n",
    "\n",
    "Most of the code described below is adapted from:\n",
    "- TextCNN (Yoon, 2014): https://arxiv.org/abs/1408.5882\n",
    "- Rationale Net (Tao et al., 2016): https://arxiv.org/abs/1606.04155\n",
    "- Extraction from Breast Pathology Reports (Yala et al., 2016): https://www.biorxiv.org/content/early/2016/10/10/079913\n",
    "\n",
    "We recommend the reader to go through these papers for having a clear understanding of the model and the theory behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Task\n",
    "\n",
    "The 20 Newsgroups dataset (http://qwone.com/~jason/20Newsgroups/) is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different categories (med, space, atheism, etc.). This dataset has been previously adopted for both clustering and classification of documents.\n",
    "\n",
    "In this tutorial, we will load the dataset through the Scikit-Learn interface and we will use it for text classification. See section 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Word Embeddings\n",
    "\n",
    "\"Word embeddings\" refers to a set of language modeling and feature learning techniques that allows computer to learn word representations in dense vectors of real numbers (as opposed to sparse co-occurrence vectors).\n",
    "\n",
    "The two most common word embedding types are Word2Vec (either Continuous Bag-of-words or Skip-Gram) and Glove, even though a number of other algorithms have been proposed through the years. In this tutorial we do not intend to describe how such algorithms work, but we suggest the reader to look at least for some basic information about them.\n",
    "\n",
    "What we can briefly mention here is that word embeddings rely on the *Distributional Hypothesis* (Harris, 1954), according to which words that occur in similar contexts tend to be similar. If we count or predict the contexts in which words occur, we can learn vectorial representations that are expected to represent similarity by mean of distance in the generated vectorial semantic space. Word vectors representing similar meaning will be closer than word vectors representing different ones.\n",
    "\n",
    "\n",
    "### 3.1 Load the Word Embeddings\n",
    "\n",
    "The first step to make our algorithm work is to provide it with word vectorial representations.\n",
    "\n",
    "One way to do so would be to collect the vocabulary used in our target dataset and learn the vectorial representations for each word from a large corpus.\n",
    "\n",
    "Another, and more practical, way consists in loading the pre-trained word embeddings, which can be easily downloaded from the Web. This is possible because we can expect that the majority of words used in our newsgroup dataset is common and frequent enough to exist in the pre-trained word embeddings. Such assumption would have been wrong if we had to deal with medical or pharmaceutical domain, as the vocabulary would have contained very rare words.\n",
    "\n",
    "In the code below, we will use Glove embeddings (https://nlp.stanford.edu/projects/glove/), but the reader can eventually use different kind of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Embeddings\n",
    "import numpy as np\n",
    "\n",
    "# Set the path where you have downloaded the embeddings\n",
    "emb_path = \"/scratch1/esantus/text_nn/data/embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "# Set the embedding size\n",
    "emb_dims = 300\n",
    "\n",
    "\n",
    "def load_embeddings(emb_path, emb_dims):\n",
    "    '''\n",
    "    Load the embeddings from a text file\n",
    "    \n",
    "        :param emb_path: Path of the text file\n",
    "        :param emb_dims: Embedding dimensions\n",
    "        \n",
    "        :return emb_tensor: tensor containing all word embeedings\n",
    "        :return word_to_indx: dictionary with word:index\n",
    "    '''\n",
    "\n",
    "    # Load the file\n",
    "    lines = open(emb_path).readlines()\n",
    "    \n",
    "    # Creating the list and adding the PADDING embedding\n",
    "    emb_tensor = [np.zeros(emb_dims)]\n",
    "    word_to_indx = {'PADDING_WORD':0}\n",
    "    \n",
    "    # For each line, save the embedding and the word:index\n",
    "    for indx, l in enumerate(lines):\n",
    "        word, emb = l.split()[0], l.split()[1:]\n",
    "        \n",
    "        if not len(emb) == emb_dims:\n",
    "            continue\n",
    "        \n",
    "        # Update the embedding list and the word:index dictionary\n",
    "        emb_tensor.append([float(x) for x in emb])\n",
    "        word_to_indx[word] = indx+1\n",
    "    \n",
    "    # Turning the list into a numpy object\n",
    "    emb_tensor = np.array(emb_tensor, dtype=np.float32)\n",
    "    return emb_tensor, word_to_indx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function load_embeddings takes in input the embedding path (pointing to a text file with one word and vector per line) and the embedding dimensions.\n",
    "\n",
    "It loads the embeddings into emb_tensor, adding a zero-padding embedding in position zero. For each word in the emb_tensor, the word index is recorded in the word_to_indx dictionary.\n",
    "\n",
    "Below we call the function and print the dimensions of both the vector tensor and dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 400001\n",
      "Vectors (+ zero-padding): (400001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Calling load_embeddings and printing the size of the returned objects\n",
    "emb_tensor, word_to_indx = load_embeddings(emb_path, emb_dims)\n",
    "\n",
    "print('Words: {}\\nVectors (+ zero-padding): {}'.format(len(word_to_indx.keys()), emb_tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Load the Dataset\n",
    "\n",
    "In section 2.0 we have shortly introduced the task. In this section we show how to load the dataset using the Scikit-Learn API.\n",
    "\n",
    "The dataset needs to be processed in a way that it can be then used by our Convolutional Neural Network for classification: our classes below are used exactly for this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Classes in the dataset\n",
    "classes = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "\n",
    "class AbstractDataset(data.Dataset):\n",
    "    '''\n",
    "    Abstract class that adds general method to the Newsgroup dataset\n",
    "    '''\n",
    "    \n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Newsgroup(AbstractDataset):\n",
    "    '''\n",
    "    Newsgroup dataset loader\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, set_type, classes, word_to_indx, class_balance_true=True, max_length=80):\n",
    "        '''\n",
    "        Load the dataset from SK-Learn\n",
    "\n",
    "            :param set_type: string containing either 'train', 'dev' or 'test'\n",
    "            :param classes: list of strings containing the classes\n",
    "            :param word_to_indx: dictionary of word:index\n",
    "            :param max_length: integer with max word to consider\n",
    "            :return: nothing\n",
    "        '''\n",
    "\n",
    "        # Deterministic randomization\n",
    "        random.seed(0)\n",
    "        \n",
    "        n_classes = len(classes)\n",
    "        class_balance = {}\n",
    "        self.dataset = []\n",
    "\n",
    "        # If train or dev...\n",
    "        if set_type in ['train', 'dev']:\n",
    "            data = self.preprocess(fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=classes))\n",
    "            \n",
    "            # Randomly split train in 80-20%\n",
    "            random.shuffle(data)\n",
    "            num_train = int(len(data)*.8)\n",
    "            if set_type == 'train':\n",
    "                data = data[:num_train]\n",
    "            else:\n",
    "                data = data[num_train:]\n",
    "                \n",
    "        # If test...     \n",
    "        else:\n",
    "            data = self.preprocess(fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=classes))\n",
    "\n",
    "        # For every unprocessed_sample in the created set, process it\n",
    "        for indx, unprocessed_sample in tqdm.tqdm(enumerate(data)):\n",
    "            sample = self.process_line(unprocessed_sample, word_to_indx, max_length)\n",
    "            \n",
    "            # If the sample is not empty, save it and add its y to the class_balance dictionary\n",
    "            if sample['text'] != '':\n",
    "                if not sample['y'] in class_balance:\n",
    "                    class_balance[sample['y']] = 0\n",
    "                class_balance[sample['y']] += 1\n",
    "                self.dataset.append(sample)\n",
    "\n",
    "            \n",
    "    def preprocess(self, data):\n",
    "        '''\n",
    "        Return a list of (text, label and label_name)\n",
    "\n",
    "            :param data: 20 newsgroup dataset as imported by SK-Learn\n",
    "            \n",
    "            :return processed_data: list of text, label and label_name\n",
    "        '''\n",
    "        processed_data = []\n",
    "        for indx, sample in enumerate(data['data']):\n",
    "            text, label = sample, data['target'][indx]\n",
    "            label_name = data['target_names'][label]\n",
    "            text = re.sub('\\W+', ' ', text).lower().strip()\n",
    "            processed_data.append((text, label, label_name))\n",
    "        return processed_data\n",
    "\n",
    "    \n",
    "    def get_indices_tensor(self, text_arr, word_to_indx, max_length):\n",
    "        '''\n",
    "        Return a tensor of max_length with the word indices\n",
    "        \n",
    "            :param text_arr: text array\n",
    "            :param word_to_indx: dictionary word:index\n",
    "            :param max_length: maximum length of returned tensors\n",
    "            \n",
    "            :return x: tensor containing the indices\n",
    "        '''\n",
    "        \n",
    "        pad_indx = 0\n",
    "        text_indx = [word_to_indx[x] if x in word_to_indx else pad_indx for x in text_arr][:max_length]\n",
    "        \n",
    "        # Padding\n",
    "        if len(text_indx) < max_length:\n",
    "            text_indx.extend([pad_indx for _ in range(max_length - len(text_indx))])\n",
    "\n",
    "        x =  torch.LongTensor([text_indx])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def process_line(self, row, word_to_indx, max_length, case_insensitive=True):\n",
    "        '''\n",
    "        Return every line as a dictionary with text, x, y, y_name\n",
    "\n",
    "            :param row: document (or comment)\n",
    "            :param word_to_indx: dictionary of word:index\n",
    "            :param max_length: integer with max word to consider\n",
    "            \n",
    "            :return sample: dictionary of text, x, y, y_name\n",
    "        '''\n",
    "        \n",
    "        text, label, label_name = row\n",
    "        \n",
    "        if case_insensitive:\n",
    "            text = \" \".join(text.split()[:max_length]).lower()\n",
    "        else:\n",
    "            text = \" \".join(text.split()[:max_length])\n",
    "            \n",
    "        x =  self.get_indices_tensor(text.split(), word_to_indx, max_length)\n",
    "        \n",
    "        sample = {'text':text,'x':x, 'y':label, 'y_name': label_name}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class AbstractDataset adds general method to the Newsgroup dataset.\n",
    "\n",
    "The class Newsgroup loads the dataset and process it, turning every line of it in a dictionary with the following keys:\n",
    "\n",
    "- text: the text of the comment\n",
    "- x: tensor containing the indices of the words in text\n",
    "- y: label (an integer)\n",
    "- y_name: name of the label\n",
    "\n",
    "Below we load the dataset in the train, dev and test sets and we print one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9051it [00:00, 10572.29it/s]\n",
      "2263it [00:00, 7198.71it/s]\n",
      "7532it [00:02, 3131.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 19, 'text': u'dr england s story deleted it was a nice read the first time through it isn t so much a matter of interpretation of bible texts that sets mormonism apart from orthodoxy as it is a matter of fabrication about 20 years ago _national lampoon_ had some comic strips in them that were drawn by neal adams they were called son o god comics it was a parody of the jesus in the bible in the comic there were a', 'y_name': 'talk.religion.misc', 'x': tensor([[ 6457,   564,  1535,   524, 16202,    21,    16,     8,  3083,  1466,\n",
      "             1,    59,    80,   132,    21, 75361,  2160,   101,   182,     8,\n",
      "          1121,     4,  6513,     4,  5490,  8239,    13,  2304, 51747,  2726,\n",
      "            26, 22887,    20,    21,    15,     8,  1121,     4, 22078,    60,\n",
      "           325,    83,   364,     0,     0,    41,    78,  4250, 11393,     7,\n",
      "           102,    13,    36,  2572,    22, 13824,  4127,    40,    36,   176,\n",
      "           631,  4869,  1534,  6109,    21,    16,     8, 13302,     4,     1,\n",
      "          3994,     7,     1,  5490,     7,     1,  4250,    64,    36,     8]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "train = Newsgroup('train', classes, word_to_indx, class_balance_true=True, max_length=80)\n",
    "dev = Newsgroup('dev', classes, word_to_indx, class_balance_true=True, max_length=80)\n",
    "test = Newsgroup('test', classes, word_to_indx, class_balance_true=True, max_length=80)\n",
    "\n",
    "# Printing 3 datapoints\n",
    "for datapoint in train[:1]:\n",
    "    print(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Define the Hyperparameters\n",
    "\n",
    "Whenever we train a neural network, a large set of hyperparameters need to be defined and tuned. Such parameters are generally tuned looking at the performance on the development set.\n",
    "\n",
    "In this section we define the default arguments. We will see in our experiments that such parameters are already good enough to obtain high accuracy on the Newsgroup dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "\n",
    "args = {'train':True, 'test':False, 'cuda':False, 'class_balance':False,\n",
    "        'init_lr':0.001, 'epochs':4, 'batch_size':128, 'patience':10,\n",
    "        'save_dir':'snapshot', 'model_path':'model.pt', 'results_path':'snapshot/results.txt', 'model':'TextCNN',\n",
    "        'hidden_dims':100, 'num_layers':1, 'dropout':0.1, 'weight_decay':1e-3,\n",
    "        'filter_num':100, 'filters':[3, 4, 5], 'num_class':20, 'emb_dims':300,\n",
    "        'tuning_metric':'loss', 'num_workers':4, 'objective':'cross_entropy'}\n",
    "\n",
    "#'gumbel_temprature':1, 'gumbel_decay':1e-5,'tag_lambda':.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Defining the Model\n",
    "\n",
    "In this section, we see how to create a Convolutional Neural Network for text classification. We do not intend here to discuss the theory behind CNNs, as the reader can easily find sources online for it (a nice tutorial can be found here: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/). We would instead propose the commented code below.\n",
    "\n",
    "Our implementation is organized in two classes:\n",
    "- one is the Encoder, which loads the embeddings, calls the model and returns the logits for the output classes;\n",
    "- the other is the model, implemented as a TextCNN, which takes in input a three dimensional tensor (batch times word_number times emb_dimensions) and returns the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the Encoder and the Model classes\n",
    "\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Load the embeddings and encode them\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embeddings, args):\n",
    "        '''\n",
    "        Load embeddings and call the TextCNN model\n",
    "        \n",
    "            :param embeddings: tensor with word embeddings\n",
    "            :param model: default is 'TextCNN'\n",
    "            \n",
    "            :return: nothing\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Saving the parameters\n",
    "        self.model = args['model']\n",
    "        self.num_class = args['num_class']\n",
    "        self.hidden_dims = args['hidden_dims']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.filters = args['filters']\n",
    "        self.filter_num = args['filter_num']\n",
    "        self.cuda = args['cuda']\n",
    "        self.dropout = args['dropout']\n",
    "        \n",
    "        # Loading the word embeddings in the Neural Network\n",
    "        vocab_size, hidden_dim = embeddings.shape\n",
    "        self.emb_dims = hidden_dim\n",
    "        self.emb_layer = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.emb_layer.weight.data = torch.from_numpy(embeddings)\n",
    "        self.emb_layer.weight.requires_grad = True\n",
    "        self.emb_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.emb_bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Calling the model, followed by a fully connected hidden layer\n",
    "        if self.model == 'TextCNN':\n",
    "            self.cnn = TextCNN(args, max_pool_over_time=True)\n",
    "            # The hidden fully connected layer size is given by the number of filters\n",
    "            # times the filter size, by the number of hidden dimensions\n",
    "            self.fc = nn.Linear(len(self.filters) * self.filter_num, hidden_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model {} not yet supported for encoder!\".format(model))\n",
    "\n",
    "        # Dropout and final layer\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.hidden = nn.Linear(hidden_dim, self.num_class)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_indx):\n",
    "        '''\n",
    "        Forward step\n",
    "        \n",
    "            :param x_indx: batch of word indices\n",
    "            \n",
    "            :return logit: predictions\n",
    "            :return: hidden layer\n",
    "        '''\n",
    "        \n",
    "        x = self.emb_layer(x_indx.squeeze(1))\n",
    "        if self.cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        # Non linear projection with dropout\n",
    "        x = F.relu(self.emb_fc(x))\n",
    "        x = self.dropout(x)\n",
    "        # TextNN, fully connected and non linearity\n",
    "        if self.model == 'TextCNN':\n",
    "            x = torch.transpose(x, 1, 2) # Transpose x dimensions into (Batch, Emb, Length)\n",
    "            hidden = self.cnn(x)\n",
    "            hidden = F.relu(self.fc(hidden))\n",
    "        else:\n",
    "            raise Exception(\"Model {} not yet supported for encoder!\".format(self.model))\n",
    "\n",
    "        # Dropout and final layer\n",
    "        hidden = self.dropout(hidden)\n",
    "        logit = self.hidden(hidden)\n",
    "        return logit, hidden\n",
    "\n",
    "\n",
    "# Model\n",
    "class TextCNN(nn.Module):\n",
    "    '''\n",
    "    CNN for Text Classification\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args, max_pool_over_time=False):\n",
    "        '''\n",
    "        Convolutional Neural Network\n",
    "        \n",
    "            :param num_layers: number of layers\n",
    "            :param filters: filters shape\n",
    "            :param filter_num: number of filters\n",
    "            :param emb_dims: embedding dimensions\n",
    "            :param max_pool_over_time: boolean\n",
    "            \n",
    "            :return: nothing\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # Saving the parameters\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.filters = args['filters']\n",
    "        self.filter_num = args['filter_num']\n",
    "        self.emb_dims = args['emb_dims']\n",
    "        self.cuda = args['cuda']\n",
    "        self.max_pool = max_pool_over_time\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        # For every layer...\n",
    "        for l in range(self.num_layers):\n",
    "            convs = []\n",
    "            \n",
    "            # For every filter...\n",
    "            for f in self.filters:\n",
    "                # Defining the sizes\n",
    "                in_channels =  self.emb_dims if l == 0 else self.filter_num * len(self.filters)\n",
    "                kernel_size = f\n",
    "                \n",
    "                # Adding the convolutions in the list\n",
    "                conv = nn.Conv1d(in_channels=in_channels, out_channels=self.filter_num, kernel_size=kernel_size)\n",
    "                self.add_module('layer_' + str(l) + '_conv_' + str(f), conv)\n",
    "                convs.append(conv)\n",
    "                \n",
    "            self.layers.append(convs)\n",
    "\n",
    "\n",
    "    def _conv(self, x):\n",
    "        '''\n",
    "        Left padding and returning the activation\n",
    "        \n",
    "            :param x: input tensor (batch, emb, length)\n",
    "            :return layer_activ: activation\n",
    "        '''\n",
    "        \n",
    "        layer_activ = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            next_activ = []\n",
    "            \n",
    "            for conv in layer:\n",
    "                # Setting the padding dimensions: it is like adding\n",
    "                # kernel_size - 1 empty embeddings\n",
    "                left_pad = conv.kernel_size[0] - 1\n",
    "                pad_tensor_size = [d for d in layer_activ.size()]\n",
    "                pad_tensor_size[2] = left_pad\n",
    "                left_pad_tensor = autograd.Variable(torch.zeros(pad_tensor_size))\n",
    "                \n",
    "                if self.cuda:\n",
    "                    left_pad_tensor = left_pad_tensor.cuda()\n",
    "                    \n",
    "                # Concatenating the padding to the tensor\n",
    "                padded_activ = torch.cat((left_pad_tensor, layer_activ), dim=2)\n",
    "                \n",
    "                # onvolution activation\n",
    "                next_activ.append(conv(padded_activ))\n",
    "\n",
    "            # Concatenating accross channels\n",
    "            layer_activ = F.relu(torch.cat(next_activ, 1))\n",
    "            #pdb.set_trace()\n",
    "        return layer_activ\n",
    "\n",
    "\n",
    "    def _pool(self, relu):\n",
    "        '''\n",
    "        Max Pool Over Time\n",
    "        '''\n",
    "        \n",
    "        pool = F.max_pool1d(relu, relu.size(2)).squeeze(-1)\n",
    "        return pool\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward steps over the x\n",
    "        \n",
    "            :param x: input (batch, emb, length)\n",
    "\n",
    "            :return activ: activation\n",
    "        '''\n",
    "        \n",
    "        activ = self._conv(x)\n",
    "        \n",
    "        # Pooling over time?\n",
    "        if self.max_pool:\n",
    "            activ = self._pool(activ)\n",
    "            \n",
    "        return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits for the first (randomly sorted) element of the dataset:\n",
      "\n",
      "\n",
      "tensor([[ 0.0795,  0.1743,  0.0644,  0.0376,  0.0002,  0.0198,  0.0035,  0.0052,\n",
      "         -0.0534, -0.0350,  0.0040,  0.0520,  0.1227, -0.0657,  0.0526, -0.0068,\n",
      "          0.0724, -0.0000,  0.0232,  0.0251]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Creating the encoder and TextCNN, and printing an output from a random input\n",
    "\n",
    "encoder = Encoder(emb_tensor, args)                    \n",
    "\n",
    "print(\"Output logits for the first (randomly sorted) element of the dataset:\\n\\n\")\n",
    "print(encoder(train[0]['x'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.0 Train the Model\n",
    "\n",
    "After loading the word embeddings and the dataset, we defined the model and the encoder. At this point, it remains to train the system and finally to evaluate it.\n",
    "\n",
    "The training code is relatively complicated, so we split it into utilities and core functions. Every function is properly commented, and we hope the reader can easily understand their goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Utilities\n",
    "\n",
    "All the functions listed below are of support for the core training functions implemented in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import sklearn.metrics\n",
    "import sys, os\n",
    "\n",
    "def get_optimizer(models, args):\n",
    "    '''\n",
    "    Save the parameters of every model in models and pass them to\n",
    "    Adam optimizer.\n",
    "    \n",
    "        :param models: list of models (such as TextCNN, etc.)\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return: torch optimizer over models\n",
    "    '''\n",
    "    params = []\n",
    "    for model in models:\n",
    "        params.extend([param for param in model.parameters() if param.requires_grad])\n",
    "    return torch.optim.Adam(params, lr=args['lr'],  weight_decay=args['weight_decay'])\n",
    "\n",
    "\n",
    "def init_metrics_dictionary(modes):\n",
    "    '''\n",
    "    Create dictionary with empty array for each metric in each mode\n",
    "    \n",
    "        :param modes: list with either train, dev or test\n",
    "        \n",
    "        :return epoch_stats: statistics for a given epoch\n",
    "    '''\n",
    "    epoch_stats = {}\n",
    "    metrics = ['loss', 'obj_loss', 'k_selection_loss', 'k_continuity_loss',\n",
    "               'accuracy', 'precision', 'recall', 'f1', 'confusion_matrix', 'mse']\n",
    "    for metric in metrics:\n",
    "        for mode in modes:\n",
    "            key = \"{}_{}\".format(mode, metric)\n",
    "            epoch_stats[key] = []\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def get_train_loader(train_data, args):\n",
    "    '''\n",
    "    Iterative train loader with sampler and replacer if class_balance\n",
    "    is true, normal otherwise.\n",
    "    \n",
    "        :param train_data: training data\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return train_loader: iterable training set\n",
    "    '''\n",
    "    \n",
    "    if args['class_balance']:\n",
    "        # If the class_balance is true: sample and replace\n",
    "        sampler = data.sampler.WeightedRandomSampler(\n",
    "                weights=train_data.weights,\n",
    "                num_samples=len(train_data),\n",
    "                replacement=True)\n",
    "        train_loader = data.DataLoader(\n",
    "                train_data,\n",
    "                num_workers=args['num_workers'],\n",
    "                sampler=sampler,\n",
    "                batch_size=args['batch_size'])\n",
    "    else:\n",
    "        # If the class_balance is false, do not sample\n",
    "        train_loader = data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=args['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=args['num_workers'],\n",
    "            drop_last=False)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_dev_loader(dev_data, args):\n",
    "    '''\n",
    "    Iterative dev loader\n",
    "    \n",
    "        :param dev_data: dev set\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return dev_loader: iterative dev set\n",
    "    '''\n",
    "    \n",
    "    dev_loader = data.DataLoader(\n",
    "        dev_data,\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=args['num_workers'],\n",
    "        drop_last=False)\n",
    "    return dev_loader\n",
    "\n",
    "\n",
    "def get_x_indx(batch, eval_model):\n",
    "    '''\n",
    "    Given a batch, return all the x\n",
    "    \n",
    "        :param batch: batch of dictionaries\n",
    "        :param eval_model: true or false, for volatile\n",
    "        \n",
    "        :return x_indx: tensor of batch*x\n",
    "    '''\n",
    "    \n",
    "    x_indx = autograd.Variable(batch['x'], volatile=eval_model)\n",
    "    return x_indx\n",
    "\n",
    "\n",
    "def get_loss(logit, y, args):\n",
    "    '''\n",
    "    Return the cross entropy or mse loss\n",
    "    \n",
    "        :param logit: predictions\n",
    "        :param y: gold standard\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return loss: loss\n",
    "    '''\n",
    "    \n",
    "    if args['objective'] == 'cross_entropy':\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "    elif args['objective'] == 'mse':\n",
    "        loss = F.mse_loss(logit, y.float())\n",
    "    else:\n",
    "        raise Exception(\"Objective {} not supported!\".format(args['objective']))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    '''\n",
    "    Return a numpy matrix from a tensor\n",
    "\n",
    "        :param tensor: tensor\n",
    "        \n",
    "        :return numpy_matrix: numpy matrix\n",
    "    '''\n",
    "    return tensor.data[0]\n",
    "\n",
    "\n",
    "def get_metrics(preds, golds, args):\n",
    "    '''\n",
    "    Return the metrics given predictions and golds\n",
    "    \n",
    "        :param preds: list of predictions\n",
    "        :param golds: list of golds\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return metrics: metrics dictionary\n",
    "    '''\n",
    "    metrics = {}\n",
    "\n",
    "    if args['objective']  in ['cross_entropy', 'margin']:\n",
    "        metrics['accuracy'] = sklearn.metrics.accuracy_score(y_true=golds, y_pred=preds)\n",
    "        metrics['confusion_matrix'] = sklearn.metrics.confusion_matrix(y_true=golds,y_pred=preds)\n",
    "        metrics['precision'] = sklearn.metrics.precision_score(y_true=golds, y_pred=preds, average=\"weighted\")\n",
    "        metrics['recall'] = sklearn.metrics.recall_score(y_true=golds,y_pred=preds, average=\"weighted\")\n",
    "        metrics['f1'] = sklearn.metrics.f1_score(y_true=golds,y_pred=preds, average=\"weighted\")\n",
    "        metrics['mse'] = \"NA\"\n",
    "    elif args['objective'] == 'mse':\n",
    "        metrics['mse'] = sklearn.metrics.mean_squared_error(y_true=golds, y_pred=preds)\n",
    "        metrics['confusion_matrix'] = \"NA\"\n",
    "        metrics['accuracy'] = \"NA\"\n",
    "        metrics['precision'] = \"NA\"\n",
    "        metrics['recall'] = \"NA\"\n",
    "        metrics['f1'] = 'NA'\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def collate_epoch_stat(stat_dict, epoch_details, mode, args):\n",
    "    '''\n",
    "    Update stat_dict with details from epoch_details and create\n",
    "    log statement\n",
    "\n",
    "        :param stat_dict: a dictionary of statistics lists to update\n",
    "        :param epoch_details: list of statistics for a given epoch\n",
    "        :param mode: train, dev or test\n",
    "        :param args: model run configuration\n",
    "\n",
    "        :return stat_dict: updated stat_dict with epoch details\n",
    "        :return log_statement: log statement sumarizing new epoch\n",
    "\n",
    "    '''\n",
    "    log_statement_details = ''\n",
    "    for metric in epoch_details:\n",
    "        loss = epoch_details[metric]\n",
    "        stat_dict['{}_{}'.format(mode, metric)].append(loss)\n",
    "\n",
    "        log_statement_details += ' -{}: {}'.format(metric, loss)\n",
    "\n",
    "    log_statement = '\\n {} - {}\\n--'.format(args['objective'], log_statement_details )\n",
    "\n",
    "    return stat_dict, log_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Core Functions\n",
    "\n",
    "Below we present the core functions for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run each epoch\n",
    "def run_epoch(data_loader, train_model, model, optimizer, step, args):\n",
    "    '''\n",
    "    Train model for one pass of train data, and return loss, acccuracy\n",
    "    \n",
    "        :param data_loader: iterable dataset\n",
    "        :param train_model: true if training, false otherwise\n",
    "        :param model: text classifier, such as TextCNN\n",
    "        :param optimizer: Adam\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return epoch_stat:\n",
    "        :return step: number of steps\n",
    "        :return losses: list of losses\n",
    "        :return preds: list of predictions\n",
    "        :return golds: list of gold standards\n",
    "    '''\n",
    "    \n",
    "    eval_model = not train_model\n",
    "    data_iter = data_loader.__iter__()\n",
    "\n",
    "    losses = []\n",
    "    obj_losses = []\n",
    "    \n",
    "    preds = []\n",
    "    golds = []\n",
    "    texts = []\n",
    "\n",
    "    if train_model:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    num_batches_per_epoch = len(data_iter)\n",
    "    if train_model:\n",
    "        num_batches_per_epoch = min(len(data_iter), 10000)\n",
    "\n",
    "    for _ in tqdm.tqdm(range(num_batches_per_epoch)):\n",
    "        # Get the batch\n",
    "        batch = data_iter.next()\n",
    "        \n",
    "        if train_model:\n",
    "            step += 1\n",
    "            #if step % 100 == 0:\n",
    "            #    args['gumbel_temprature'] = max(np.exp((step+1) * -1 * args['gumbel_decay']), .05)\n",
    "\n",
    "        # Load X and Y\n",
    "        x_indx = get_x_indx(batch, eval_model)\n",
    "        text = batch['text']\n",
    "        y = autograd.Variable(batch['y'], volatile=eval_model)\n",
    "\n",
    "        if args['cuda']:\n",
    "            x_indx, y = x_indx.cuda(), y.cuda()\n",
    "\n",
    "        if train_model:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logit, _ = model(x_indx)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = get_loss(logit, y, args)\n",
    "        obj_loss = loss\n",
    "\n",
    "        # Backward step\n",
    "        if train_model:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Saving loss\n",
    "        obj_losses.append(tensor_to_numpy(obj_loss))\n",
    "        losses.append(tensor_to_numpy(loss))\n",
    "        \n",
    "        # Softmax, preds, text and gold\n",
    "        batch_softmax = F.softmax(logit, dim=-1).cpu()\n",
    "        preds.extend(torch.max(batch_softmax, 1)[1].view(y.size()).data.numpy())\n",
    "        texts.extend(text)\n",
    "        golds.extend(batch['y'].numpy())\n",
    "\n",
    "    # Get metrics\n",
    "    epoch_metrics = get_metrics(preds, golds, args)\n",
    "    epoch_stat = {'loss' : np.mean(losses), 'obj_loss': np.mean(obj_losses)}\n",
    "\n",
    "    for metric_k in epoch_metrics.keys():\n",
    "        epoch_stat[metric_k] = epoch_metrics[metric_k]\n",
    "\n",
    "    return epoch_stat, step, losses, preds, golds\n",
    "\n",
    "\n",
    "def train_model(train_data, dev_data, model, args):\n",
    "    '''\n",
    "    Train model on the training and tune it on the dev set.\n",
    "    \n",
    "    If model does not improve dev performance within patience\n",
    "    epochs, best model is restored and the learning rate halved\n",
    "    to continue training.\n",
    "\n",
    "    At the end of training, the function will restore the best model\n",
    "    on the dev set.\n",
    "\n",
    "        :param train_data: preprocessed data\n",
    "        :param dev_data: preprocessed data\n",
    "        :param models: models to be used for text classification\n",
    "        :param args: hyperparameters\n",
    "        \n",
    "        :return epoch_stats: a dictionary of metrics for train and dev\n",
    "        :return model: best model\n",
    "    '''\n",
    "    \n",
    "    snapshot = '{}'.format(os.path.join(args['save_dir'], args['model_path']))\n",
    "\n",
    "    if args['cuda']:\n",
    "        model = model.cuda()\n",
    "\n",
    "    args['lr'] = args['init_lr']\n",
    "    optimizer = get_optimizer([model], args)\n",
    "\n",
    "    num_epoch_sans_improvement = 0\n",
    "    epoch_stats = init_metrics_dictionary(modes=['train', 'dev'])\n",
    "    step = 0\n",
    "    tuning_key = \"dev_{}\".format(args['tuning_metric'])\n",
    "    best_epoch_func = min if tuning_key == 'loss' else max\n",
    "\n",
    "    train_loader = get_train_loader(train_data, args)\n",
    "    dev_loader = get_dev_loader(dev_data, args)\n",
    "\n",
    "    # For every epoch...\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "        \n",
    "        # Load the training and dev sets...\n",
    "        for mode, dataset, loader in [('Train', train_data, train_loader),\n",
    "                                      ('Dev', dev_data, dev_loader)]:\n",
    "            \n",
    "            train_model = mode == 'Train'\n",
    "            print('{}'.format(mode))\n",
    "            key_prefix = mode.lower()\n",
    "            epoch_details, step, _, _, _ = run_epoch(data_loader=loader, train_model=train_model, model=model,\n",
    "                                                     optimizer=optimizer, step=step, args=args)\n",
    "            \n",
    "            epoch_stats, log_statement = collate_epoch_stat(epoch_stats, epoch_details, key_prefix, args)\n",
    "            \n",
    "            # Log performance\n",
    "            print(log_statement)\n",
    "\n",
    "        # Save model if beats best dev\n",
    "        best_func = min if args['tuning_metric'] == 'loss' else max\n",
    "        if best_func(epoch_stats[tuning_key]) == epoch_stats[tuning_key][-1]:\n",
    "            num_epoch_sans_improvement = 0\n",
    "            if not os.path.isdir(args['save_dir']):\n",
    "                os.makedirs(args['save_dir'])\n",
    "            # Subtract one because epoch is 1-indexed and arr is 0-indexed\n",
    "            epoch_stats['best_epoch'] = epoch - 1\n",
    "            torch.save(model, snapshot)\n",
    "        else:\n",
    "            num_epoch_sans_improvement += 1\n",
    "\n",
    "        if not train_model:\n",
    "            print('---- Best Dev {} is {:.4f} at epoch {}'.format(\n",
    "                args['tuning_metric'], epoch_stats[tuning_key][epoch_stats['best_epoch']],\n",
    "                epoch_stats['best_epoch'] + 1))\n",
    "\n",
    "        # If the number of epochs without improvements is high, reduce the learning rate\n",
    "        if num_epoch_sans_improvement >= args['patience']:\n",
    "            print(\"Reducing learning rate\")\n",
    "            num_epoch_sans_improvement = 0\n",
    "            model.cpu()\n",
    "            model = torch.load(snapshot)\n",
    "\n",
    "            if args['cuda']:\n",
    "                model = model.cuda()\n",
    "            args['lr'] *= .5\n",
    "            optimizer = get_optimizer([model], args)\n",
    "\n",
    "    # Restore model to best dev performance\n",
    "    if os.path.exists(args['model_path']):\n",
    "        model.cpu()\n",
    "        model = torch.load(snapshot)\n",
    "\n",
    "    return epoch_stats, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Epoch 1:\n",
      "\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:133: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 69/69 [01:26<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 2.53454446793 -f1: 0.16526570441 -recall: 0.172939169983 -precision: 0.198678071704 -obj_loss: 2.53454446793 -mse: NA -confusion_matrix: [[  7   3   2  39   0  21   0   2   6   2   3  74   3  64   6  85   8  58\n",
      "    0   1]\n",
      " [  0  83  35 128  40  80  17   8   8   1   0  49   7  14   7   2   0   3\n",
      "    0   0]\n",
      " [  1  61  85 143  19  62  22   5   4   0   0  24   7   9   1   1   1   0\n",
      "    0   0]\n",
      " [  2  52  27 168  34  51  58   8   2   0   0  22  21   5   1   2   0   2\n",
      "    0   0]\n",
      " [  1  46  22 163  53  40  43   6   5   1   3  20  23  11   4   0   2   0\n",
      "    0   0]\n",
      " [  2  76  45 131  21  64  17   3   3   2   1  70  10  11   8   1   1   4\n",
      "    0   0]\n",
      " [  0  48  11 144  23  26  87  26   8   6   8  32  14  15   7   2   0   3\n",
      "    0   0]\n",
      " [  1  19   5  51   6  19  39  95  32   6  16  92   6  41   2   2   1  11\n",
      "    0   0]\n",
      " [  5   9   5  53   2  25  22  64  49  10  12  88   9  56   7  11   9  24\n",
      "    1   0]\n",
      " [  0   4   2  42   0  22   4   8   8 117 101  88   3  36   1   5   0  10\n",
      "    0   1]\n",
      " [  2   2   3  47   0  16  10   5   4 104 124  89   3  45   0   3   2   6\n",
      "    0   0]\n",
      " [  4  36  10  85  12  49  10   5   7   1   1 172   7  41   9  17   5   8\n",
      "    0   0]\n",
      " [  1  38   8 118  27  36  41  20  11   3   3  66  37  21   9   1   4   2\n",
      "    0   0]\n",
      " [ 13  11   4  51   4  27   7   5  18   8   7  95   6 111  11  37  11  23\n",
      "    0   1]\n",
      " [  0  16   7  61   4  22  11  25  26   5   4 123  25  49  31  17   7  13\n",
      "    0   0]\n",
      " [ 19   7   5  30   0  19   0   1   3   7   7 105   2  93   4  93   7  69\n",
      "    0   3]\n",
      " [ 10   4   0  40   1  21   5  11  22  10  12  90   6  59  12  43  24  51\n",
      "    0   2]\n",
      " [  9   1   3  35   0  22   0   3   3   5   5  98   4  77   4  56  12 119\n",
      "    0   2]\n",
      " [ 19   3   4  37   0  19   2   3   8  14   9  69   9  53   9  42  16  52\n",
      "    0   0]\n",
      " [ 11   2   4  28   0  17   1   2   2   5   6  69   1  44   3  44   5  44\n",
      "    0   2]] -accuracy: 0.172939169983\n",
      "--\n",
      "Dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:101: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.90it/s]\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TextCNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.83517348766 -f1: 0.297038599759 -recall: 0.348574015392 -precision: 0.424342240077 -obj_loss: 1.83517348766 -mse: NA -confusion_matrix: [[ 63   0   0   0   0   0   0   1   1   0   0   2   1  12   1   2   1   0\n",
      "    0   0]\n",
      " [  1  28   5  24   0   2   0   0   1   0   0  19   4   2   1   0   0   0\n",
      "    0   0]\n",
      " [  1  19  51  21   0   5   0   1   1   0   0   9   7   6   1   0   1   0\n",
      "    0   0]\n",
      " [  0  10   4  82   0   0   3   0   1   0   0   7  10   4   1   0   1   0\n",
      "    0   0]\n",
      " [  3  10   1  64   0   1   2   1   2   0   0   8  21   0   1   0   0   0\n",
      "    0   0]\n",
      " [  1  48  15   7   0  10   0   0   0   0   0  28   6   4   1   0   0   0\n",
      "    0   0]\n",
      " [  3   3   4  38   0   0  17   6  10   3   0   4  21   7   1   0   0   0\n",
      "    0   0]\n",
      " [  6   0   0   1   0   0   1  28  64   0   0   1   8   6   1   0   2   0\n",
      "    0   0]\n",
      " [  6   0   0   2   0   0   0  11  79   3   0   2   7   7   2   0   2   0\n",
      "    1   0]\n",
      " [  2   0   0   0   0   0   1   1   2 104   0   1   1   4   1   0   3   1\n",
      "    2   0]\n",
      " [  3   0   0   0   0   0   1   0   5 101   2   1   0   4   0   0   2   0\n",
      "    0   0]\n",
      " [ 12   5   0   4   0   2   0   0   1   0   0  56   7  14   1   0   1   0\n",
      "    0   0]\n",
      " [  1   2   1  27   0   1   4   3   8   1   0   5  57  12   6   0   0   0\n",
      "    0   0]\n",
      " [ 10   1   0   0   0   0   0   0   0   1   0   4   1 109   1   0   1   0\n",
      "    0   0]\n",
      " [  8   4   0   3   0   1   0   0   9   2   0   8  14  42  39   0   1   0\n",
      "    0   0]\n",
      " [ 92   0   0   0   0   0   0   0   1   0   0   1   0   9   0  12   0   0\n",
      "    1   0]\n",
      " [ 36   1   0   0   0   0   0   1  12   2   0   5   4  39   1   1   4   1\n",
      "    2   0]\n",
      " [ 54   0   0   0   0   0   0   0   0   1   0   1   0   3   0   0   2  28\n",
      "    0   0]\n",
      " [ 52   0   0   0   0   0   0   0   7   2   0   3   1  14   0   0   3   1\n",
      "    1   0]\n",
      " [ 49   0   0   0   0   2   0   0   1   0   0   0   0  13   0   6   0   0\n",
      "    0   0]] -accuracy: 0.348574015392\n",
      "--\n",
      "---- Best Dev loss is 1.8352 at epoch 1\n",
      "-------------\n",
      "Epoch 2:\n",
      "\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:24<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.55445182323 -f1: 0.455755274881 -recall: 0.471290505969 -precision: 0.448998919191 -obj_loss: 1.55445182323 -mse: NA -confusion_matrix: [[ 56   0   0   0   0   3   1   3   9   2   3  14   2  18   9 137  36  44\n",
      "   21  26]\n",
      " [  1 175  40  35  23 103  19   5   3   0   1  41  12   4  12   0   6   0\n",
      "    2   0]\n",
      " [  0  38 218  33  22  67  21   2   4   2   2  13  12   2   5   0   2   0\n",
      "    1   1]\n",
      " [  2  40  46 157  92   6  54   4   3   0   1   9  33   0   3   1   4   0\n",
      "    0   0]\n",
      " [  0  52  13 121 102  13  54   5   4   0   0  10  51   3  12   0   0   2\n",
      "    1   0]\n",
      " [  1 105  80   7   3 191   5   0   6   1   0  40   9   1   8   0   9   1\n",
      "    3   0]\n",
      " [  2  11  19  52  29   7 192  41  18   4   8   6  45   1  15   1   8   0\n",
      "    1   0]\n",
      " [  4   1   3   1   1   2  19 225 122   1   2   5  19   6  10   0  19   3\n",
      "    1   0]\n",
      " [  4   0   2   3   3   3  15 130 194   2   9   7  11  13   9   2  44   3\n",
      "    7   0]\n",
      " [  1   1   1   0   0   2   9   3   4 311  85   1   0   2   1   4  17   7\n",
      "    3   0]\n",
      " [  0   0   0   0   0   2   7   2   9  67 358   1   0   2   4   1   8   1\n",
      "    3   0]\n",
      " [  9  19   2   5   7  10   1   5   6   0   1 318  15   8  16   5  21  13\n",
      "   16   2]\n",
      " [  0  18   6  34  41   4  53  24  11   2   3  20 185   7  31   1   3   1\n",
      "    2   0]\n",
      " [  8   1   0   2   0   3   3   0   6   2   4   6   9 355  22   3  17   3\n",
      "    5   1]\n",
      " [  2   5   1   0   0   9   5   5  17   4   1  21  21  23 281   6  28   4\n",
      "    8   5]\n",
      " [ 52   0   0   0   1   0   0   0   1   3   1   6   2  14   4 314  14  18\n",
      "   12  32]\n",
      " [ 18   2   2   1   0   3   3   8  36  11  10  24   6  20  22  19 149  50\n",
      "   33   6]\n",
      " [ 20   1   0   0   0   1   0   1   7   6   1  10   0  10   5  19  32 324\n",
      "   20   1]\n",
      " [ 36   2   1   1   0   2   3   6  10   4   4  23   3  44  11  32 100  56\n",
      "   28   2]\n",
      " [ 41   0   0   0   0   1   2   1   4   5   0  10   0  25   5 129  26  15\n",
      "   14  12]] -accuracy: 0.471290505969\n",
      "--\n",
      "Dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.33249115944 -f1: 0.526683862115 -recall: 0.559529198732 -precision: 0.524419672961 -obj_loss: 1.33249115944 -mse: NA -confusion_matrix: [[  7   0   0   0   0   1   0   1   2   0   0   8   0   3   2  47   4   4\n",
      "    5   0]\n",
      " [  0  24   7  11   0  32   4   0   0   1   0   5   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5  56  19   0  29   6   3   0   0   0   2   1   1   0   0   1   0\n",
      "    0   0]\n",
      " [  1   5   9  84   0   4   3   0   1   0   0   3  12   0   0   0   1   0\n",
      "    0   0]\n",
      " [  1   2   4  73   0   9   6   4   1   0   0   4   8   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   3  11   4   0  88   3   0   1   0   0   5   2   0   1   0   1   1\n",
      "    0   0]\n",
      " [  0   0   6  23   0   1  59   6   5   2   2   2   6   2   0   1   1   0\n",
      "    1   0]\n",
      " [  2   0   0   0   0   2   2  86   8   2   1   1   3   1   0   0   4   2\n",
      "    4   0]\n",
      " [  1   0   0   3   0   2   1  32  64   5   1   1   5   3   0   1   1   1\n",
      "    1   0]\n",
      " [  1   0   0   0   0   2   3   0   1 101   7   0   1   3   0   0   3   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   1   1   0   1  11  96   1   0   2   0   1   2   0\n",
      "    3   0]\n",
      " [  0   1   1   3   0   3   0   0   0   0   0  78   3   1   3   1   3   2\n",
      "    4   0]\n",
      " [  0   2   2  16   0   5   9   6   2   1   1   5  75   2   1   0   1   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   4   0   0   0   1   1   3   2 111   0   2   1   0\n",
      "    2   0]\n",
      " [  1   0   1   2   0   4   2   2   3   2   0   5   9  11  79   2   2   1\n",
      "    5   0]\n",
      " [  3   0   0   1   0   1   1   0   1   0   0   0   0   6   1  97   0   3\n",
      "    2   0]\n",
      " [  5   0   0   1   0   0   0   1   3   4   0  11   2   5   2   2  44  12\n",
      "   17   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   1   2   3   0   0   3   4  73\n",
      "    0   0]\n",
      " [  7   0   0   0   0   0   0   4   3   4   0   6   1   6   1  12  14  12\n",
      "   14   0]\n",
      " [  7   0   1   0   0   1   0   0   1   0   0   1   0   7   1  42   3   2\n",
      "    5   0]] -accuracy: 0.559529198732\n",
      "--\n",
      "---- Best Dev loss is 1.3325 at epoch 2\n",
      "-------------\n",
      "Epoch 3:\n",
      "\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:24<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.22113788128 -f1: 0.583429658366 -recall: 0.594656054576 -precision: 0.580692381593 -obj_loss: 1.22113788128 -mse: NA -confusion_matrix: [[ 94   0   0   0   3   2   2   5   7   4   2   7   3  12   2 130  23  28\n",
      "   45  15]\n",
      " [  1 217  36  24  25 121   7   2   7   1   1  12   8   4  12   0   3   0\n",
      "    1   0]\n",
      " [  1  31 252  34  16  61  13   2   1   5   0  13  12   0   3   1   0   0\n",
      "    0   0]\n",
      " [  1  27  56 178  99   5  35   3   1   0   1   6  36   3   1   1   1   0\n",
      "    1   0]\n",
      " [  1  32  19 134 143  11  32   5   1   1   0   4  50   3   5   0   1   0\n",
      "    1   0]\n",
      " [  4  72  48   3   5 300   8   0   1   0   0  15   0   1   7   1   2   0\n",
      "    3   0]\n",
      " [  0   8   8  48  14   9 283  24   9   3   6   3  27   3  10   2   2   0\n",
      "    1   0]\n",
      " [  3   0   3   0   2   5  18 279  67   1   3   6  24   3   3   2  18   1\n",
      "    6   0]\n",
      " [  3   1   2   2   1   5  12  77 283   4  11   3  10  10   7   2  18   0\n",
      "   10   0]\n",
      " [  2   0   0   0   0   2   5   0   5 378  36   1   0   2   1   3   6   5\n",
      "    6   0]\n",
      " [  1   1   0   0   0   2   4   2  11  24 406   0   1   3   2   2   5   0\n",
      "    1   0]\n",
      " [  6   8   5   4   4  12   0   3   3   0   0 361  18   3   2   1  19  10\n",
      "   20   0]\n",
      " [  0  14   6  19  35   3  31  20   5   4   1  18 264   3  16   1   5   0\n",
      "    1   0]\n",
      " [  7   2   0   0   2   5   5   0   9   0   1   1  10 369  10   3   4   1\n",
      "   13   8]\n",
      " [  3   6   2   0   0  10   2   5  16   4   1  13  15  20 315   9  13   1\n",
      "    7   4]\n",
      " [ 39   0   0   0   1   1   1   0   3   3   1   6   0   8   4 363   3   7\n",
      "   14  20]\n",
      " [ 16   0   2   1   0   4   0  14  15   3   6  14   4   7  13  12 250  16\n",
      "   46   0]\n",
      " [ 23   1   0   0   1   1   2   2   1   8   3   6   0   6   1   8  28 339\n",
      "   28   0]\n",
      " [ 35   1   1   0   0   3   1   6   7   6   3  22   1  28   7  10  52  39\n",
      "  141   5]\n",
      " [ 42   0   1   0   0   2   0   2   7   3   0   3   1  20   6 138  18  16\n",
      "   16  15]] -accuracy: 0.594656054576\n",
      "--\n",
      "Dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 18/18 [00:03<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.20884215832 -f1: 0.590639694225 -recall: 0.604345857854 -precision: 0.620687982038 -obj_loss: 1.20884215832 -mse: NA -confusion_matrix: [[ 24   1   0   0   0   2   1   0   2   1   0   5   0   0   3  36   5   2\n",
      "    2   0]\n",
      " [  1  53   7   1   7   9   3   0   0   1   0   1   0   1   2   1   0   0\n",
      "    0   0]\n",
      " [  1  16  61   7   7  21   5   4   0   0   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0  15  11  26  59   2   3   1   0   0   0   2   2   0   0   1   1   0\n",
      "    0   0]\n",
      " [  2  17   4   9  65   2   7   6   0   1   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  28   5   0   2  78   0   2   0   0   0   2   0   0   1   0   1   1\n",
      "    0   0]\n",
      " [  1   0   5   5   9   1  71   9   1   3   1   1   3   1   3   1   2   0\n",
      "    0   0]\n",
      " [  4   0   0   0   1   1   5  93   3   2   0   0   1   1   0   0   4   1\n",
      "    2   0]\n",
      " [  2   2   0   0   1   2   4  34  61   5   2   0   3   1   2   0   2   0\n",
      "    1   0]\n",
      " [  1   0   1   0   0   1   2   0   0 107   6   1   0   3   0   0   1   0\n",
      "    0   0]\n",
      " [  1   0   1   0   0   1   1   1   1  13  94   0   0   2   0   0   3   0\n",
      "    1   0]\n",
      " [  0   2   0   1   2   4   0   0   0   0   0  78   2   0   3   1   7   0\n",
      "    3   0]\n",
      " [  0   8   3   0  18   1  11  10   2   1   1   3  60   1   7   0   2   0\n",
      "    0   0]\n",
      " [  2   0   1   0   0   4   0   1   0   1   0   0   0 109   3   2   0   0\n",
      "    3   2]\n",
      " [  4   3   0   0   2   2   1   3   0   2   0   0   2   3 102   1   4   0\n",
      "    1   1]\n",
      " [  9   0   0   1   0   1   0   1   0   0   0   0   0   5   1  94   2   2\n",
      "    0   0]\n",
      " [  6   0   0   0   1   1   1   5   0   3   0  11   0   4   1   1  66   2\n",
      "    6   1]\n",
      " [  5   0   0   0   1   1   0   0   0   0   1   1   0   0   1   1   4  74\n",
      "    0   0]\n",
      " [ 12   0   0   0   0   0   0   7   0   2   1   5   0   3   3   5  23   6\n",
      "   17   0]\n",
      " [ 14   0   2   0   0   1   0   3   0   0   0   0   0   1   4  38   4   2\n",
      "    0   2]] -accuracy: 0.604345857854\n",
      "--\n",
      "---- Best Dev loss is 1.2088 at epoch 3\n",
      "-------------\n",
      "Epoch 4:\n",
      "\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:24<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.05057942867 -f1: 0.649351925721 -recall: 0.658328595793 -precision: 0.647481145886 -obj_loss: 1.05057942867 -mse: NA -confusion_matrix: [[133   0   0   0   0   1   2   5   7   2   2   6   1   8   1 114  17  22\n",
      "   47  16]\n",
      " [  1 289  34  19  15  82   9   3   2   1   0   5  10   4   8   0   0   0\n",
      "    0   0]\n",
      " [  0  38 285  37  20  33   9   2   2   4   1   9   2   1   1   1   0   0\n",
      "    0   0]\n",
      " [  1  20  56 168 126   5  36   5   0   0   1   1  30   1   1   2   1   1\n",
      "    0   0]\n",
      " [  1  32  16  99 226   5  25   2   1   0   0   2  29   3   1   0   1   0\n",
      "    0   0]\n",
      " [  2  80  37   5   2 314   2   0   0   1   2   9   5   1   4   2   1   2\n",
      "    1   0]\n",
      " [  0  11  10  32  16   1 303  22  10   3   4   1  30   2   8   2   2   0\n",
      "    3   0]\n",
      " [  2   0   1   1   1   3  13 324  44   2   2   4  19   2   1   2  17   0\n",
      "    5   1]\n",
      " [  3   3   3   2   2   1   7  60 316   5   7   2   9   7   8   2  14   1\n",
      "    9   0]\n",
      " [  4   0   0   0   0   1   5   0   5 400  19   1   0   1   1   1   3   3\n",
      "    8   0]\n",
      " [  0   0   3   0   0   1   6   2   8  23 413   0   2   0   3   1   1   0\n",
      "    2   0]\n",
      " [  2   6   8   2   3   4   0   3   2   0   0 381  12   1   4   1  14   7\n",
      "   28   1]\n",
      " [  0  14   2  16  37   2  31  16   4   3   1  15 295   1   7   1   1   0\n",
      "    0   0]\n",
      " [  9   3   2   0   0   2   2   1   6   0   3   1   4 386  15   2   2   0\n",
      "   10   2]\n",
      " [  4   6   4   0   2   8   0   6   9   1   0   9  11  15 341   5   9   3\n",
      "   11   2]\n",
      " [ 53   0   1   0   0   3   1   1   1   2   0   0   2   7   5 369   2   6\n",
      "   14   7]\n",
      " [ 17   1   2   1   0   3   0   9  13   4   3  10   5   3  10   9 275   4\n",
      "   50   4]\n",
      " [ 18   1   1   0   1   0   0   0   1   9   2   6   0   5   2   6  17 360\n",
      "   27   2]\n",
      " [ 28   1   0   0   0   3   3   2  10   6   4  20   1  10   7   7  43  17\n",
      "  199   7]\n",
      " [ 67   0   1   0   0   3   0   1   3   2   2   2   0  18   8 124  22  10\n",
      "   14  13]] -accuracy: 0.658328595793\n",
      "--\n",
      "Dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.14141225815 -f1: 0.620506942851 -recall: 0.631507469443 -precision: 0.624740772783 -obj_loss: 1.14141225815 -mse: NA -confusion_matrix: [[  2   1   0   0   0   2   1   0   1   0   1   1   1   0   3  38   2   2\n",
      "   11  18]\n",
      " [  1  37   7   9   6  14   5   0   0   1   0   1   2   1   2   1   0   0\n",
      "    0   0]\n",
      " [  0   3  66  16   3  22   6   3   2   0   0   0   0   1   0   0   0   0\n",
      "    0   1]\n",
      " [  0   4   9  77  12   1   6   0   1   0   0   1  10   1   0   0   1   0\n",
      "    0   0]\n",
      " [  0   4   6  42  36   3   7   3   2   0   0   1   7   0   2   1   0   0\n",
      "    0   0]\n",
      " [  0   5   7   2   1  97   1   0   1   0   0   0   3   0   2   0   0   1\n",
      "    0   0]\n",
      " [  0   0   5  11   0   0  84   2   3   2   1   0   5   0   2   0   2   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   1   8  82   9   2   1   0   3   1   0   0   3   0\n",
      "    6   1]\n",
      " [  1   1   0   0   0   0   6  21  73   3   4   1   5   1   1   0   3   0\n",
      "    1   1]\n",
      " [  1   0   0   0   0   1   2   0   3  96  12   0   1   4   0   0   0   0\n",
      "    2   1]\n",
      " [  0   0   0   0   0   1   1   1   2   3 104   0   0   2   0   1   2   0\n",
      "    2   0]\n",
      " [  0   0   1   1   1   2   0   0   0   0   0  75   6   1   4   1   3   0\n",
      "    8   0]\n",
      " [  0   3   4   4   5   2  13   8   2   1   1   2  78   0   2   0   3   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   4   0   1   1   1   1   0   1 107   1   2   0   1\n",
      "    4   4]\n",
      " [  1   1   0   1   0   1   4   2   4   2   1   0   4   3  99   2   2   0\n",
      "    2   2]\n",
      " [  1   0   0   1   0   1   0   0   1   0   0   0   0   5   1  88   1   2\n",
      "    2  13]\n",
      " [  1   0   0   0   1   1   0   5   2   2   1   3   1   3   0   0  70   1\n",
      "   11   7]\n",
      " [  3   0   0   1   0   0   0   0   0   0   2   0   0   0   2   3   3  71\n",
      "    0   4]\n",
      " [  2   0   0   0   1   0   0   3   1   2   1   2   2   2   3   3  15   2\n",
      "   38   7]\n",
      " [  1   0   1   0   0   1   0   2   2   0   0   0   1   1   3  31   7   1\n",
      "    5  15]] -accuracy: 0.631507469443\n",
      "--\n",
      "---- Best Dev loss is 1.1414 at epoch 4\n"
     ]
    }
   ],
   "source": [
    "epoch_stats, model = train_model(train, dev, encoder, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained only for 4 epochs for time limitations. Yet, its performance on the dev set is very high (its accuracy reaches 91% at the fourth epoch). We can therefore proceed to evaluate the TextCNN on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8.0 Test the Model\n",
    "\n",
    "In the previous sections, we have loaded the embeddings and the dataset, we have set the hyperparameters and we have implemented the model as a CNN. Finally we have trained its parameters and we are now at the point in which we have to test its performance.\n",
    "\n",
    "In the following section we describe the code for testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "def test_model(test_data, model, args):\n",
    "    '''\n",
    "    Run the model on test data, and return statistics,\n",
    "    including loss and accuracy.\n",
    "    \n",
    "        :param test_data: test data\n",
    "        :param model: a model, like TextCNN\n",
    "        :param args: arguments\n",
    "        \n",
    "        :return test_stats:\n",
    "    '''\n",
    "    if args['cuda']:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Loading the test data as iterable\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=args['num_workers'],\n",
    "        drop_last=False)\n",
    "\n",
    "    # The function is defined before\n",
    "    test_stats = init_metrics_dictionary(modes=['test'])\n",
    "\n",
    "    mode = 'Test'\n",
    "    train_model = False\n",
    "    key_prefix = mode.lower()\n",
    "    print(\"-------------\\nTest\")\n",
    "    epoch_details, _, losses, preds, golds = run_epoch(\n",
    "        data_loader=test_loader,\n",
    "        train_model=train_model,\n",
    "        model=model,\n",
    "        optimizer=None,\n",
    "        step=None,\n",
    "        args=args)\n",
    "\n",
    "    test_stats, log_statement = collate_epoch_stat(test_stats, epoch_details, 'test', args)\n",
    "    test_stats['losses'] = losses\n",
    "    test_stats['preds'] = preds\n",
    "    test_stats['golds'] = golds\n",
    "\n",
    "    print(log_statement)\n",
    "\n",
    "    return test_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple function calls the run_epoch over the test set and return the metric statistics, which are finally printed in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:101: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:133: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 58/58 [00:11<00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cross_entropy -  -loss: 1.2976808548 -f1: 0.585272234341 -recall: 0.595567109044 -precision: 0.596501985905 -obj_loss: 1.2976808548 -mse: NA -confusion_matrix: [[  9   0   1   0   0   9   4   2   9   2   3   5   3   9  19 147  13   2\n",
      "   21  53]\n",
      " [  0 184  22  11  19  76  23   2   7   1   0   8  13   3   9   1   0   2\n",
      "    1   2]\n",
      " [  0  14 205  47  14  52   8   4   6   1   1   3   3   4   4   3   4   0\n",
      "    2   4]\n",
      " [  1   3  34 212  47   9  26   3   3   3   2   1  38   1   1   0   1   0\n",
      "    0   0]\n",
      " [  1   3   9 137 121   2  35   2   3   1   3   3  39   6   3   1   1   0\n",
      "    0   1]\n",
      " [  0  34  29  12   8 265  12   2   1   4   0   7   7   1   7   0   0   1\n",
      "    0   0]\n",
      " [  0   1   7  35   5   1 281  14   6   2   3   1  13   2   4   2   3   0\n",
      "    1   1]\n",
      " [  0   2   0   0   0   2  20 260  37   2   4   2  18   2   6   2  14   0\n",
      "    1   2]\n",
      " [  0   1   2   4   4   3  12  34 263   6   3   1  12   7   7   3   8   1\n",
      "    8   7]\n",
      " [  1   1   0   1   0   6   4   0   7 289  46   1   3   6   0   1   3   1\n",
      "    8   4]\n",
      " [  0   1   0   1   0   2   2   1   9  11 350   1   1   2   1   4   1   1\n",
      "    1   1]\n",
      " [  1   0   1   5   9   7   4   0   3   3   0 247  25   8  11   3  20   2\n",
      "   23   8]\n",
      " [  1   4  10  23  21   9  34  14   9   3   2  10 210  14   9   1   2   2\n",
      "    2   2]\n",
      " [  0   3   0   2   0   1   2   0  14   4   3   2  11 303  10   8   2   1\n",
      "    9   7]\n",
      " [  0   5   0   1   2   4   8   3   3   3   3   2  27  10 269   5   5   1\n",
      "   15  11]\n",
      " [  0   0   1   2   0   6   1   0   3   1   1   0   0   4   5 309   4   4\n",
      "    8  35]\n",
      " [  3   0   1   0   0   5   1  10  13   4   1   6   4   5  13  12 212   3\n",
      "   41  19]\n",
      " [ 27   0   0   0   0   1   0   2   8   6   0   2   1   5   1  17  15 225\n",
      "   27  33]\n",
      " [  2   0   0   0   1   3   3   4   5   6   1   4   4  13  11  11  91   5\n",
      "  114  25]\n",
      " [  0   0   1   0   0   2   2   5   7   3   0   2   4   9   8 134  31   2\n",
      "    9  25]] -accuracy: 0.595567109044\n",
      "--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = test_model(test, model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9.0 Conclusions\n",
    "\n",
    "In this tutorial we have seen how to develop TextCNN, a Convolutional Neural Network for text classification. Despite the fact that this model can reach a very good performance on the 20 Newsgroup dataset with few epochs of training, modifications and improvements are necessary when dealing with more complex datasets. \n",
    "\n",
    "Some of them may include using character and positional embeddings, adding multiple layers, including recurrent neural network layers, attention, etc. We leave to the reader the joy of trying new approaches, also exploring the related literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
